{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./dataset/train.pkl','rb')\n",
    "train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17534\n"
     ]
    }
   ],
   "source": [
    "ID = set()\n",
    "for i in train.keys():\n",
    "    ID.add(i[:-2])\n",
    "#print(phone)\n",
    "ID = list(ID)\n",
    "#print(\"###########################\")\n",
    "print(len(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取label为1的训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ID:\n",
    "    a = train[i+'-A']\n",
    "    b = train[i+'-B']\n",
    "    X.append([a,b])\n",
    "    Y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取label为0的训练数据\n",
    "\n",
    "+ 不平衡比例为3：1，此类数据的数量约为50000组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(ID)\n",
    "for i in ID[:1000]:\n",
    "    random.shuffle(ID) \n",
    "    index = 0\n",
    "    for j in ID:\n",
    "        if i != j :\n",
    "            a = train[i+'-A']\n",
    "            b = train[j+'-B']\n",
    "            X.append([a,b])\n",
    "            Y.append(0)\n",
    "            index += 1\n",
    "        \n",
    "        if index == 50:         \n",
    "            break\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67534"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = shuffle(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Siamese模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.models import Model\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAX_A_LENGTH = 100\n",
    "MAX_B_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    \n",
    "    l2 = 2\n",
    "\n",
    "    return (\n",
    "             1.1 * ( y_true - 0.0 ) * K.square(K.maximum(0.0, l2 - y_pred)) +\n",
    "             ( 1.0 - y_true ) * K.square(y_pred)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义的距离函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def exp_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.exp( 2.5 - K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True)))\n",
    "\n",
    "def dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入\n",
    "A_sequence_input = Input(shape=(MAX_A_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "B_sequence_input = Input(shape=(MAX_B_LENGTH,EMBEDDING_DIM), dtype='float32')\n",
    "\n",
    "# 双向LSTM\n",
    "shared_lstm = Bidirectional(LSTM(MAX_A_LENGTH, return_sequences = False))\n",
    "\n",
    "BN_A = BatchNormalization()(A_sequence_input)\n",
    "encoded_A = shared_lstm(BN_A)\n",
    "\n",
    "BN_B = BatchNormalization()(B_sequence_input)\n",
    "encoded_B = shared_lstm(BN_B)\n",
    "\n",
    "encoded_A = Dropout(rate=0.1)(encoded_A)\n",
    "encoded_A = Dense(units=128)(encoded_A)\n",
    "\n",
    "\n",
    "encoded_B = Dropout(rate=0.1)(encoded_B)\n",
    "encoded_B = Dense(units=128)(encoded_B)\n",
    "\n",
    "distance = Lambda(exp_distance,output_shape=dist_output_shape)([encoded_A, encoded_B])\n",
    "\n",
    "model = Model(inputs=[A_sequence_input, B_sequence_input], outputs=distance)    \n",
    "\n",
    "# optimizer\n",
    "rms = RMSprop()\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss=contrastive_loss,  # 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100, 100)     400         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100, 100)     400         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          160800      batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          25728       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          25728       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 213,056\n",
      "Trainable params: 212,656\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 打印Model的结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练(以生成器的方式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = keras.utils.to_categorical(np.asarray(Y),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "def generate_batch_data_random(x, y, batch_size):\n",
    "    \"\"\"逐步提取batch数据到显存，降低对内存的占用\"\"\"\n",
    "    \n",
    "    #word2vec模型导入\n",
    "    w2v_model = word2vec.load('./text8.bin')\n",
    "    \n",
    "    while (True):\n",
    "        \n",
    "        x,y = shuffle(x,y)\n",
    "        \n",
    "        for i in range(batch_size,len(x),batch_size):\n",
    "            \n",
    "            batch_A = []\n",
    "            batch_B = []\n",
    "            batch_y = y[i - batch_size:i]\n",
    "            \n",
    "            for item in x[i - batch_size:i]:\n",
    "                \n",
    "                seq = []\n",
    "                for line in item[0][-1]:    \n",
    "                    tmp = []\n",
    "                    for word in line.strip().split(' '):\n",
    "                        if word in w2v_model.vocab:\n",
    "                            tmp.append(w2v_model[word])\n",
    "                    seq.append((len(tmp) * np.mean(tmp,0)).tolist())\n",
    "                batch_A.append(seq)\n",
    "            \n",
    "                seq = []\n",
    "                for line in item[1][-1]:    \n",
    "                    tmp = []\n",
    "                    for word in line.strip().split(' '):\n",
    "                        if word in w2v_model.vocab:\n",
    "                            tmp.append(w2v_model[word])\n",
    "                    seq.append((len(tmp) * np.mean(tmp,0)).tolist())\n",
    "                batch_B.append(seq) \n",
    "            \n",
    "            yield [np.asarray(batch_A),np.asarray(batch_B)],batch_y\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65/65 [==============================] - 599s 9s/step - loss: 0.0048 - acc: 0.7388\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 580s 9s/step - loss: 0.0035 - acc: 0.7400\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 578s 9s/step - loss: 0.0030 - acc: 0.7403\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 585s 9s/step - loss: 0.0026 - acc: 0.7406\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 578s 9s/step - loss: 0.0022 - acc: 0.7403\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 576s 9s/step - loss: 0.0020 - acc: 0.7402\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 576s 9s/step - loss: 0.0018 - acc: 0.7400\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 578s 9s/step - loss: 0.0018 - acc: 0.7408\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 577s 9s/step - loss: 0.0015 - acc: 0.7404\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 598s 9s/step - loss: 0.0014 - acc: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9484b1c18>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "epoch = 10\n",
    "model.fit_generator(\n",
    "    generate_batch_data_random(X,Y,batch_size),\n",
    "    steps_per_epoch = len(X)//batch_size,\n",
    "    epochs = epoch,\n",
    "    verbose=1,\n",
    "    callbacks=[TensorBoard(log_dir='./log/')]    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./dataset/test.pkl','rb')\n",
    "test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1948\n"
     ]
    }
   ],
   "source": [
    "ID = set()\n",
    "for i in test.keys():\n",
    "    ID.add(i[:-2])\n",
    "#print(ID)\n",
    "ID = list(ID)\n",
    "#print(\"###########################\")\n",
    "print(len(ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_Y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取label为1的训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ID:\n",
    "    a = test[i+'-A']\n",
    "    b = test[i+'-B']\n",
    "    test_X.append([a,b])\n",
    "    test_Y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1948"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取label为0的训练数据\n",
    "\n",
    "+ 不平衡比例为3：1，此类数据的数量约为6000组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(ID)\n",
    "for i in ID[:600]:\n",
    "    random.shuffle(ID) \n",
    "    index = 0\n",
    "    for j in ID:\n",
    "        if i != j :\n",
    "            a = test[i+'-A']\n",
    "            b = test[j+'-B']\n",
    "            test_X.append([a,b])\n",
    "            test_Y.append(0)\n",
    "            index += 1\n",
    "        \n",
    "        if index == 10:         \n",
    "            break\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7948"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X,test_Y = shuffle(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec\n",
    "#word2vec模型导入\n",
    "w2v_model = word2vec.load('./text8.bin')\n",
    "\n",
    "def generate_batch_test_data_random(x):\n",
    "    \"\"\"逐步提取batch数据到显存，降低对显存的占用\"\"\"\n",
    "                \n",
    "    batch_A = []\n",
    "    batch_B = []\n",
    "\n",
    "    for item in x:\n",
    "\n",
    "        seq = []\n",
    "        for line in item[0][-1]:    \n",
    "            tmp = []\n",
    "            for word in line.strip().split(' '):\n",
    "                if word in w2v_model.vocab:\n",
    "                    tmp.append(w2v_model[word])\n",
    "            seq.append((len(tmp) * np.mean(tmp,0)).tolist())\n",
    "        batch_A.append(seq)\n",
    "\n",
    "        seq = []\n",
    "        for line in item[1][-1]:    \n",
    "            tmp = []\n",
    "            for word in line.strip().split(' '):\n",
    "                if word in w2v_model.vocab:\n",
    "                    tmp.append(w2v_model[word])\n",
    "            seq.append((len(tmp) * np.mean(tmp,0)).tolist())\n",
    "        batch_B.append(seq) \n",
    "\n",
    "    return [np.asarray(batch_A),np.asarray(batch_B)]\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "batch_size = 1000\n",
    "for i in range(batch_size,len(test_X),batch_size):\n",
    "        \n",
    "    batch_pred = model.predict(generate_batch_test_data_random(test_X[i-batch_size:i]))\n",
    "    for res,unit in zip(batch_pred,test_X[i-batch_size:i]):\n",
    "        \n",
    "        threshold = 2.4\n",
    "        if res > threshold:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5293    0]\n",
      " [   0 1707]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred\n",
    "\n",
    "y_true = test_Y[:7000]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_true,y_pred)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      5293\n",
      "          1       1.00      1.00      1.00      1707\n",
      "\n",
      "avg / total       1.00      1.00      1.00      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9d4d20438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.cm as cm \n",
    "from sklearn.metrics import confusion_matrix \n",
    "import numpy as np\n",
    "\n",
    " \n",
    "labels = [0,1] \n",
    "cm = confusion_matrix(y_true, y_pred,labels=labels)  \n",
    "plt.matshow(cm)  \n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')  \n",
    "plt.xlabel('Predicted label')  \n",
    "plt.xticks(np.arange(cm.shape[1]),labels)  \n",
    "plt.yticks(np.arange(cm.shape[1]),labels)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
